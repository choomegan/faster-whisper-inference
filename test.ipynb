{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversion of transformer model to ct2:\n",
    "\n",
    "\n",
    "ct2-transformers-converter --model /mnt/c/Projects/models/whisper/whisper-full-ft-no-PL/checkpoint-2353 --output_dir /mnt/c/Projects/models/whisper/whisper-full-ft-no-PL-ct2 --quantization float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/peft/main/en/package_reference/lora#peft.LoraModel.merge_and_unload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"/mnt/c/Projects/models/whisper/whisper-full-ft-no-PL-ct2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Projects/code/faster-whisper-inference/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
    "\n",
    "model = WhisperModel(model_dir, device=\"cuda\", compute_type=\"float16\")\n",
    "batched_model = BatchedInferencePipeline(model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data = \"/mnt/c/Projects/datasets/PORT/set_1_2_test_manifest.json\"\n",
    "base_dir = os.path.dirname(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def load_manifest_nemo(input_manifest_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    loads the manifest file in Nvidia NeMo format to process the entries and store them\n",
    "    into a list of dictionaries\n",
    "\n",
    "    input_manifest_path: the manifest path that contains the information of the audio\n",
    "    clips of interest\n",
    "    ---\n",
    "    returns: a list of dictionaries of the information in the input manifest file\n",
    "    \"\"\"\n",
    "\n",
    "    dict_list = []\n",
    "\n",
    "    with open(input_manifest_path, \"r+\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            dict_list.append(json.loads(line))\n",
    "\n",
    "    return dict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_manifest_nemo(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Projects/datasets/PORT'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df:\n",
    "    item['audio_filepath'] = os.path.join(base_dir, item['audio_filepath'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'audio_filepath': '/mnt/c/Projects/datasets/PORT/mms_set_1/test_split/mms/test/CHDIR_154_2022-04-04_18-00000000-00007597.wav',\n",
       "  'root_file': 'CHDIR_154_2022-04-04_18.wav',\n",
       "  'duration': 7.597,\n",
       "  'start': 0.0,\n",
       "  'end': 7.597,\n",
       "  'text': 'TUGBOAT SOL TEN ELEVEN TUGBOAT SOL TEN ELEVEN MOTOR TANKER ACHILLEAS IS CALLING ON SEVEN THREE'},\n",
       " {'audio_filepath': '/mnt/c/Projects/datasets/PORT/mms_set_1/test_split/mms/test/CHDIR_154_2022-04-04_18-00007597-00012685.wav',\n",
       "  'root_file': 'CHDIR_154_2022-04-04_18.wav',\n",
       "  'duration': 5.088,\n",
       "  'start': 7.597,\n",
       "  'end': 12.685,\n",
       "  'text': 'SOL ONE ZERO ONE ONE V T I S WEST SOL ONE ZERO ONE ONE'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_filepaths = []\n",
    "\n",
    "for item in df:\n",
    "    audio_filepaths.append(item['audio_filepath'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<av.InputContainer '/mnt/c/Projects/datasets/PORT/mms_set_1/test_split/mms/test/CHDIR_154_2022-04-04_18-00000000-00007597.wav'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import av\n",
    "av.open('/mnt/c/Projects/datasets/PORT/mms_set_1/test_split/mms/test/CHDIR_154_2022-04-04_18-00000000-00007597.wav') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Projects/datasets/PORT/mms_set_1/test_split/mms/test/CHDIR_154_2022-04-04_18-00007597-00012685.wav'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_filepaths[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "segments, info = model.transcribe(audio_filepaths[1], beam_size=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File object has no read() method, or readable() returned False.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m segments, info \u001b[38;5;241m=\u001b[39m \u001b[43mbatched_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranscribe\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43maudio_filepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Projects/code/faster-whisper-inference/venv/lib/python3.10/site-packages/faster_whisper/transcribe.py:388\u001b[0m, in \u001b[0;36mBatchedInferencePipeline.transcribe\u001b[0;34m(self, audio, language, task, log_progress, beam_size, best_of, patience, length_penalty, repetition_penalty, no_repeat_ngram_size, temperature, compression_ratio_threshold, log_prob_threshold, no_speech_threshold, condition_on_previous_text, prompt_reset_on_temperature, initial_prompt, prefix, suppress_blank, suppress_tokens, without_timestamps, max_initial_timestamp, word_timestamps, prepend_punctuations, append_punctuations, multilingual, vad_filter, vad_parameters, max_new_tokens, chunk_length, clip_timestamps, hallucination_silence_threshold, batch_size, hotwords, language_detection_threshold, language_detection_segments)\u001b[0m\n\u001b[1;32m    385\u001b[0m     multilingual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(audio, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m--> 388\u001b[0m     audio \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampling_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m duration \u001b[38;5;241m=\u001b[39m audio\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m sampling_rate\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing audio with duration \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, format_timestamp(duration)\n\u001b[1;32m    393\u001b[0m )\n",
      "File \u001b[0;32m/mnt/c/Projects/code/faster-whisper-inference/venv/lib/python3.10/site-packages/faster_whisper/audio.py:46\u001b[0m, in \u001b[0;36mdecode_audio\u001b[0;34m(input_file, sampling_rate, split_stereo)\u001b[0m\n\u001b[1;32m     43\u001b[0m raw_buffer \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[1;32m     44\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mav\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m container:\n\u001b[1;32m     47\u001b[0m     frames \u001b[38;5;241m=\u001b[39m container\u001b[38;5;241m.\u001b[39mdecode(audio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m     frames \u001b[38;5;241m=\u001b[39m _ignore_invalid_frames(frames)\n",
      "File \u001b[0;32m/mnt/c/Projects/code/faster-whisper-inference/venv/lib/python3.10/site-packages/av/container/core.pyx:418\u001b[0m, in \u001b[0;36mav.container.core.open\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt/c/Projects/code/faster-whisper-inference/venv/lib/python3.10/site-packages/av/container/core.pyx:264\u001b[0m, in \u001b[0;36mav.container.core.Container.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/mnt/c/Projects/code/faster-whisper-inference/venv/lib/python3.10/site-packages/av/container/pyio.pyx:41\u001b[0m, in \u001b[0;36mav.container.pyio.PyIOFile.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: File object has no read() method, or readable() returned False."
     ]
    }
   ],
   "source": [
    "segments, info = batched_model.transcribe(audio = audio_filepaths[:1], batch_size=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
